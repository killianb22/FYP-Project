# -*- coding: utf-8 -*-
"""FYP_EDA_CODE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nZuFYCMCvwKGG_2VfIZZ88n2J5edCYLE

# Creating dataset - Using Geo Location
"""

from google.colab import drive
drive.mount('/content/gdrive')

lexical_fields = {
    'inflation with economic terms': ['inflation', 'hyperinflation', 'deflation', 'stagflation', 'price index',
                                      'monetary policy', 'purchasing power','deflationary', 'anti-inflationary', 'anti-deflationary'],

    'expensive': ['Expensive', 'expensive', 'prohibitive', 'costly', 'high', 'exorbitant', 'unaffordable', 'consequential',
                  'inaccessible', 'excessive', 'abnormal', 'expensive', 'rip-off', 'rip-off', 'ruinous', 'outrageous', 'out of reach',
                  'roundabout', 'inconceivable', 'prohibitive'],

    'cheap': ['Low', 'modest', 'advantageous','discounted', 'unbeatable', 'derisory','attractive', 'bargain', 'bargain price',
              'attractive', 'bargain', 'affordable','reasonable', 'competitive','accessible', 'acceptable', 'normal',
              'fair', 'interesting', 'suitable','negligible'],

    'prices_costs': ['price', 'cost', 'expense', 'fee', 'charge', 'rate', 'tariff','sale', 'purchase', 'lease', 'fee',
                     'subscription', 'bill', 'cost', 'charge', 'pay', 'rate', 'sell', 'quote', 'payment','discount'],

    'statistical_institutions': ['Bureau of Labor Statistics', 'Consumer Price Index', 'Federal Reserve',
                                  'ECB', 'central bank', 'Banque de France', 'INSEE', 'FED', 'rate', 'interest rate', 'Central Bank of Ireland',
                                 'Bank of England', 'Bank', 'Investment Institution'],

    'additional_keywords': ['economy', 'market', 'value', 'money', 'finance',]
}

import os
import json
import gzip  # Import gzip for handling .gz (gzipped) files

# json_dir is the directory that contains a day's worth of files
json_dir = '/content/gdrive/MyDrive/November 21st 2022'

outfolder = 'output'
if not os.path.exists(outfolder):
    os.makedirs(outfolder)

json_list = []  # list of lists, each sublist has 1 string element = 1 line

for path, dirs, files in os.walk(json_dir):
    for f in files:
        if f.endswith('.json') or f.endswith('.gz'):  # Check for .json and .gz files
            json_file = os.path.join(path, f)
            if f.endswith('.gz'):  # If the file is a .gz file, decompress it first
                with gzip.open(json_file, 'rt', encoding='utf-8') as jf:  # Open and read the .gz file
                    jfile_list = list(jf)
                    json_list.extend(jfile_list)
            else:
                with open(json_file, 'r', encoding='utf-8') as jf:
                    jfile_list = list(jf)
                    json_list.extend(jfile_list)

print("Finished reading", len(json_list), 'records into list')
print('Converting geocoded JSONL records to dictionary now...')

geo_dict = {}  # dictionary of dicts, each dict has line parsed into keys / values
i = 0
for json_str in json_list:
    result = json.loads(json_str)  # convert line / string to dict
    if result.get('geo') is not None:  # only take records that were geocoded
        geo_dict[result['id']] = result
    i += 1
    if i % 100000 == 0:
        print('Processed', i, 'records...')

print('Finished processing', i, 'records.')
print('Created dictionary with', len(geo_dict), 'geocoded records...')

# Define the output file name
outfile = 'all_records_with_geo_v21.json'

# Correctly construct the output path by joining the output folder and the output file name
outpath = os.path.join(outfolder, outfile)

print('Writing output for all geo records...')
with open(outpath, 'w', encoding='utf-8') as outf:
    json.dump(geo_dict, outf)

print('Wrote output file - Done!')

import json, csv, os

# The input file is already correctly specified with its full path.
infile = '/content/output/all_records_with_geo_v21.json'

# Load the JSON data directly from the infile
with open(infile, 'r', encoding='utf-8') as json_file:
    twit_data = json.load(json_file)

twit_list = []

# Process the JSON data to extract the needed information.
for k, v in twit_data.items():
    tweet_id = k
    timestamp = v.get('created_at')
    tweet = v.get('text')
    lang = v.get('lang')
    longitude = v.get('geo')['coordinates'][1] if v.get('geo') else None
    latitude = v.get('geo')['coordinates'][0] if v.get('geo') else None
    country = v.get('place')['country'] if v.get('place') else None
    ccode = v.get('place')['country_code'] if v.get('place') else None

    record = [tweet_id, timestamp, tweet, lang, longitude, latitude, country, ccode]
    twit_list.append(record)


outfile = 'november_21st.csv'

# Ensure the output directory exists before writing the file.
output_dir = 'output'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

outpath = os.path.join(output_dir, outfile)

# Write the output CSV file.
with open(outpath, 'w', newline='', encoding='utf-8') as writefile:
    writer = csv.writer(writefile, quoting=csv.QUOTE_ALL, delimiter=',')
    header = ['tweet_id', 'timestamp', 'tweet', 'lang', 'longitude', 'latitude', 'country','ccode']
    writer.writerow(header)
    writer.writerows(twit_list)

print('Done!')

"""# Creating dataset"""

import pandas as pd

# Columns to keep
columns_to_keep = ['tweet_id', 'timestamp', 'tweet', 'lang', 'longitude', 'latitude', 'country', 'ccode']

# Load the datasets
df1 = pd.read_csv('/content/EDA/November 1st 2022.csv', usecols=lambda x: x in columns_to_keep)
df2 = pd.read_csv('/content/EDA/November 2nd 2022.csv', usecols=lambda x: x in columns_to_keep)
df3 = pd.read_csv('/content/EDA/November 3rd 2022.csv', usecols=lambda x: x in columns_to_keep)
df4 = pd.read_csv('/content/EDA/November 4th 2022.csv', usecols=lambda x: x in columns_to_keep)
df5 = pd.read_csv('/content/EDA/November 5th 2022.csv', usecols=lambda x: x in columns_to_keep)
df6 = pd.read_csv('/content/EDA/November 6th 2022.csv', usecols=lambda x: x in columns_to_keep)
df7 = pd.read_csv('/content/EDA/November 7th 2022.csv', usecols=lambda x: x in columns_to_keep)
df8 = pd.read_csv('/content/EDA/november_8th.csv', usecols=lambda x: x in columns_to_keep)
df9 = pd.read_csv('/content/EDA/november_9th.csv', usecols=lambda x: x in columns_to_keep)
df10 = pd.read_csv('/content/EDA/november_10th.csv', usecols=lambda x: x in columns_to_keep)
df11 = pd.read_csv('/content/EDA/november_11th.csv', usecols=lambda x: x in columns_to_keep)
df12 = pd.read_csv('/content/EDA/november_12th.csv', usecols=lambda x: x in columns_to_keep)
df13 = pd.read_csv('/content/EDA/november_13th.csv', usecols=lambda x: x in columns_to_keep)
df14 = pd.read_csv('/content/EDA/november_14th.csv', usecols=lambda x: x in columns_to_keep)
df15 = pd.read_csv('/content/EDA/november_15th.csv', usecols=lambda x: x in columns_to_keep)
df16 = pd.read_csv('/content/EDA/november_16th.csv', usecols=lambda x: x in columns_to_keep)
df17 = pd.read_csv('/content/EDA/november_17th.csv', usecols=lambda x: x in columns_to_keep)
df18 = pd.read_csv('/content/EDA/november_18th.csv', usecols=lambda x: x in columns_to_keep)
df19 = pd.read_csv('/content/EDA/november_19th.csv', usecols=lambda x: x in columns_to_keep)
df20 = pd.read_csv('/content/EDA/november_20th.csv', usecols=lambda x: x in columns_to_keep)
df21 = pd.read_csv('/content/EDA/november_21st.csv', usecols=lambda x: x in columns_to_keep)

# Merge the dataframes
merged_df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20, df21 ])

# Save the merged dataframe to a new CSV file
merged_df.to_csv('/content/EDA_Combined.csv', index=False)

# Output the path to the merged file
merged_file_path = '/content/Combined EDA .csv'
merged_file_path

"""# Preprocessing"""

import pandas as pd

# Load the dataset
file_path = '/content/EDA_Combined.csv'
df = pd.read_csv(file_path)

# Check the first few rows of the dataframe to understand its structure
df.head()

# Function to check if a tweet contains any of the keywords
def contains_keywords(tweet, keywords):
    # Ensure the tweet is a string
    tweet = str(tweet)
    return any(keyword.lower() in tweet.lower() for keyword in keywords)

# Count the tweets containing each category of keywords
keyword_counts = {category: sum(df['tweet'].apply(lambda tweet: contains_keywords(tweet, keywords))) for category, keywords in lexical_fields.items()}

keyword_counts

print(df.shape)

print(df.isna().sum())

df = df.dropna()

# Function to check if a tweet contains any of the keywords from the lexical fields
def filter_tweets(tweet, fields):
    if isinstance(tweet, str):
        return any(any(keyword in tweet.lower() for keyword in field) for field in fields.values())
    else:
        # If tweet is not a string, we do not consider it for our lexical fields
        return False

import re
# Apply the adjusted filter function to the DataFrame
filtered_df = df[df['tweet'].apply(lambda tweet: filter_tweets(tweet, lexical_fields))]

# Function to identify invalid characters
def contains_invalid_characters(text):
    # Define a set of allowed punctuation characters
    allowed_punctuation = "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"
    pattern = r'^[A-Za-z0-9\s' + re.escape(allowed_punctuation) + r']+$'
    return not re.match(pattern, text)

# Apply the function to the 'clean_text' column to create a mask for rows with invalid characters
invalid_mask = filtered_df['tweet'].apply(lambda x: contains_invalid_characters(str(x)))

# Invert the mask to filter out the invalid rows
df = filtered_df[~invalid_mask]

# Now 'clean_df' contains only rows with valid characters & english language
print(f"Number of rows after removing invalid characters: {len(df)}")

unique_country = df['ccode'].unique()

print("Unique country in the 'ccode' column:")
for country in unique_country:
    print(country)

# Count of all countries in the 'ccode' column
country_counts = df['ccode'].value_counts()

country_counts

import matplotlib.pyplot as plt

# Plotting the count of countries in 'ccode'
plt.figure(figsize=(15, 10))
country_counts[:20].plot(kind='bar')  # Only plotting the top 20 for readability
plt.title('Top 20 Countries by Count in CCode')
plt.xlabel('Country Code')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

"""## Add sentiment column - Unsupervised"""

import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# Convert all tweet texts to strings and handle missing values
df['tweet'] = df['tweet'].astype(str).fillna('')

# Initialize the VADER sentiment intensity analyzer
sia = SentimentIntensityAnalyzer()

# Define a function to get numerical sentiment
def get_numerical_sentiment(text):
    score = sia.polarity_scores(text)
    # Return a numerical sentiment based on the compound score
    if score['compound'] >= 0.05:
        return 1  # Positive
    elif score['compound'] <= -0.05:
        return -1  # Negative
    else:
        return 0  # Neutral

# Apply the numerical sentiment function to each tweet
df['sentiment'] = df['tweet'].apply(get_numerical_sentiment)

# Save the modified DataFrame to a new CSV file
new_csv_path = '/content/EDA_Combined_Sent_Inf.csv'
df.to_csv(new_csv_path, index=False)

print(f"New CSV file saved to: {new_csv_path}")

"""# EDA"""

import pandas as pd
df_sent = pd.read_csv("/content/EDA_Combined_Sent_Inf.csv")

df_sent.head()

print(df_sent.columns)

# Find the number of positive and negative reviews
print('Number of positive and negative reviews: ', df_sent.sentiment.value_counts())

# Find the proportion of positive and negative reviews
print('Proportion of positive and negative reviews: ', df_sent.sentiment.value_counts() / len(df))

"""## Sentiment Distribution"""

import matplotlib.pyplot as plt
import seaborn as sns

sns.countplot(x='sentiment', data= df_sent)
plt.title('Distribution of Sentiments')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()

"""## Tweet Language Distribution"""

# Tweet Language Distribution
top_n = 10
languages_count = df_sent['lang'].value_counts()
top_languages = languages_count[:top_n]
top_languages['Other'] = languages_count[top_n:].sum()

plt.figure(figsize=(10, 8))
top_languages.plot(kind='bar')
plt.title('Tweet Language Distribution')
plt.xlabel('Language')
plt.ylabel('Number of Tweets')
plt.xticks(rotation=45)
plt.show()

"""## Tweets Over Time"""

df_sent['timestamp'] = pd.to_datetime(df_sent['timestamp'])
df_sent.set_index('timestamp', inplace=True)

plt.figure(figsize=(12, 6))
df_sent.resample('D').size().plot(label='Total tweets per day', color='blue')
plt.title('Tweets Over Time')
plt.xlabel('Time')
plt.ylabel('Number of Tweets')
plt.legend()
plt.show()

"""## Sentiment Over Time"""

# Group by the index
sentiment_over_time = df_sent.groupby([df_sent.index.date, 'sentiment']).size().unstack(fill_value=0)

# Plotting
sentiment_over_time.plot(kind='line', figsize=(12, 6))
plt.title('Sentiment Over Time')
plt.xlabel('Time')
plt.ylabel('Number of Tweets')
plt.legend(['Negative', 'Neutral', 'Positive'], title='Sentiment')
plt.xticks(rotation=45)
plt.show()

"""### Sentiment in US over time"""

# Filter the dataset for US only, considering the original datetime index
df_us_corrected = df_sent[df_sent['country'] == "United States"]

# Group by the index for US dataset
sentiment_over_time_us_corrected = df_us_corrected.groupby([df_us_corrected.index.date, 'sentiment']).size().unstack(fill_value=0)

# Plotting for US data
sentiment_over_time_us_corrected.plot(kind='line', figsize=(12, 6))
plt.title('Sentiment Over Time in the US')
plt.xlabel('Time')
plt.ylabel('Number of Tweets')
plt.legend(['Negative', 'Neutral', 'Positive'], title='Sentiment')
plt.xticks(rotation=45)
plt.show()

"""## Geographical Distribution of Tweets with a World map"""

!pip install geopandas

import geopandas as gpd

# Load a simple world map
world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))

# Create a GeoDataFrame from tweet data
gdf = gpd.GeoDataFrame(df_sent, geometry=gpd.points_from_xy(df_sent.longitude, df_sent.latitude))

# Plot
fig, ax = plt.subplots(figsize=(15, 10))
world.plot(ax=ax, color='lightgrey')
gdf.plot(ax=ax, markersize=5, color='red', alpha=0.5)
plt.title('Geographical Distribution of Tweets')
plt.show()

"""## Language vs Sentiment"""

# Focus on Top Languages by tweet count
top_languages = df_sent['lang'].value_counts().nlargest(5).index

# Filter data for these top languages
filtered_data = df_sent[df_sent['lang'].isin(top_languages)]

# Pivot table for this filtered data
pivot_table_filtered = filtered_data.pivot_table(index='lang', columns='sentiment', aggfunc='size', fill_value=0)


plt.figure(figsize=(12, 8))
ax = sns.heatmap(pivot_table_filtered, annot=True, cmap='coolwarm', fmt='d', annot_kws={"size": 10})
plt.title('Language vs. Sentiment for Top 5 Languages', fontsize=14)
plt.xlabel('Sentiment', fontsize=12)
plt.ylabel('Language', fontsize=12)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.show()

"""# Advanced EDA

## How does average sentiment change over time for the top languages?
"""

# Filter for top languages to make the plot more readable
top_languages = df_sent['lang'].value_counts().head(5).index
filtered_data = df_sent[df_sent['lang'].isin(top_languages)]

# Group data by 'timestamp' (index) and 'lang', then calculate the average sentiment
daily_sentiment = filtered_data.groupby([pd.Grouper(freq='D'), 'lang'])['sentiment'].mean().unstack()

# Plotting
plt.figure(figsize=(12, 6))
sns.lineplot(data=daily_sentiment, dashes=False)
plt.title('Daily Sentiment Trends by Language')
plt.xlabel('Date')
plt.ylabel('Average Sentiment')
plt.legend(title='Language')
plt.xticks(rotation=45)
plt.show()

"""## Geospatial Heat Maps of Sentiment"""

# Convert DataFrame to a GeoDataFrame
gdf = gpd.GeoDataFrame(df_sent, geometry=gpd.points_from_xy(df_sent.longitude, df_sent.latitude))

# Load a basemap
world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))

# Plotting
fig, ax = plt.subplots(figsize=(15, 10))
world.plot(ax=ax, color='lightgrey')


gdf.plot(ax=ax, marker='o', column='sentiment', cmap='coolwarm', alpha=0.5, legend=True, markersize=10)

plt.title('Geospatial Heat Maps of Sentiment')
plt.show()

"""## Average sentiment numbers per country"""

# Aggregate average sentiment by country
avg_sentiment_per_country = gdf.groupby('country')['sentiment'].mean().reset_index()

# Merge this average sentiment data with the world GeoDataFrame
world_with_sentiment = world.merge(avg_sentiment_per_country, how='left', left_on='name', right_on='country')

# Plot
fig, ax = plt.subplots(figsize=(15, 10))
base = world_with_sentiment.plot(ax=ax, column='sentiment', cmap='coolwarm', missing_kwds={'color': 'lightgrey'}, legend=True)
world.boundary.plot(ax=ax, edgecolor='k', linewidth=0.5)
ax.set_title('Average Sentiment by Country')
plt.show()