{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "iRQThIbhF8fI",
        "sBoNZCmAGG3N"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Google Colab attempt - Using Geo Location"
      ],
      "metadata": {
        "id": "mM5FewrOHO9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "ZAl0Rbf8Da31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lexical_fields = {\n",
        "    'inflation with economic terms': ['inflation', 'hyperinflation', 'deflation', 'stagflation', 'price index',\n",
        "                                      'monetary policy', 'purchasing power','deflationary', 'anti-inflationary', 'anti-deflationary'],\n",
        "\n",
        "    'expensive': ['Expensive', 'expensive', 'prohibitive', 'costly', 'high', 'exorbitant', 'unaffordable', 'consequential',\n",
        "                  'inaccessible', 'excessive', 'abnormal', 'expensive', 'rip-off', 'rip-off', 'ruinous', 'outrageous', 'out of reach',\n",
        "                  'roundabout', 'inconceivable', 'prohibitive'],\n",
        "\n",
        "    'cheap': ['Low', 'modest', 'advantageous','discounted', 'unbeatable', 'derisory','attractive', 'bargain', 'bargain price',\n",
        "              'attractive', 'bargain', 'affordable','reasonable', 'competitive','accessible', 'acceptable', 'normal',\n",
        "              'fair', 'interesting', 'suitable','negligible'],\n",
        "\n",
        "    'prices_costs': ['price', 'cost', 'expense', 'fee', 'charge', 'rate', 'tariff','sale', 'purchase', 'lease', 'fee',\n",
        "                     'subscription', 'bill', 'cost', 'charge', 'pay', 'rate', 'sell', 'quote', 'payment','discount'],\n",
        "\n",
        "    'statistical_institutions': ['Bureau of Labor Statistics', 'Consumer Price Index', 'Federal Reserve',\n",
        "                                  'ECB', 'central bank', 'Banque de France', 'INSEE', 'FED', 'rate', 'interest rate', 'Central Bank of Ireland',\n",
        "                                 'Bank of England', 'Bank', 'Investment Institution'],\n",
        "\n",
        "    'additional_keywords': ['economy', 'market', 'value', 'money', 'finance',]\n",
        "}"
      ],
      "metadata": {
        "id": "CMUeLibACki3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import gzip  # Import gzip for handling .gz (gzipped) files\n",
        "\n",
        "# json_dir is the directory that contains a day's worth of files\n",
        "# This directory should be below the directory where this script is stored:\n",
        "json_dir = '/content/gdrive/MyDrive/November 21st 2022'\n",
        "\n",
        "outfolder = 'output'\n",
        "if not os.path.exists(outfolder):\n",
        "    os.makedirs(outfolder)\n",
        "\n",
        "json_list = []  # list of lists, each sublist has 1 string element = 1 line\n",
        "\n",
        "for path, dirs, files in os.walk(json_dir):\n",
        "    for f in files:\n",
        "        if f.endswith('.json') or f.endswith('.gz'):  # Check for .json and .gz files\n",
        "            json_file = os.path.join(path, f)\n",
        "            if f.endswith('.gz'):  # If the file is a .gz file, decompress it first\n",
        "                with gzip.open(json_file, 'rt', encoding='utf-8') as jf:  # Open and read the .gz file\n",
        "                    jfile_list = list(jf)\n",
        "                    json_list.extend(jfile_list)\n",
        "            else:  # For .json files, process as before\n",
        "                with open(json_file, 'r', encoding='utf-8') as jf:\n",
        "                    jfile_list = list(jf)\n",
        "                    json_list.extend(jfile_list)\n",
        "\n",
        "print(\"Finished reading\", len(json_list), 'records into list')\n",
        "print('Converting geocoded JSONL records to dictionary now...')\n",
        "\n",
        "geo_dict = {}  # dictionary of dicts, each dict has line parsed into keys / values\n",
        "i = 0\n",
        "for json_str in json_list:\n",
        "    result = json.loads(json_str)  # convert line / string to dict\n",
        "    if result.get('geo') is not None:  # only take records that were geocoded\n",
        "        geo_dict[result['id']] = result\n",
        "    i += 1\n",
        "    if i % 100000 == 0:\n",
        "        print('Processed', i, 'records...')\n",
        "\n",
        "print('Finished processing', i, 'records.')\n",
        "print('Created dictionary with', len(geo_dict), 'geocoded records...')\n",
        "\n",
        "# Define the output file name\n",
        "outfile = 'all_records_with_geo_v21.json'\n",
        "\n",
        "# Correctly construct the output path by joining the output folder and the output file name\n",
        "outpath = os.path.join(outfolder, outfile)\n",
        "\n",
        "print('Writing output for all geo records...')\n",
        "with open(outpath, 'w', encoding='utf-8') as outf:\n",
        "    json.dump(geo_dict, outf)\n",
        "\n",
        "print('Wrote output file - Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22cF_8le3FmW",
        "outputId": "5b979f16-c9e3-4e02-80eb-2d0bfc231fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished reading 4190001 records into list\n",
            "Converting geocoded JSONL records to dictionary now...\n",
            "Processed 100000 records...\n",
            "Processed 200000 records...\n",
            "Processed 300000 records...\n",
            "Processed 400000 records...\n",
            "Processed 500000 records...\n",
            "Processed 600000 records...\n",
            "Processed 700000 records...\n",
            "Processed 800000 records...\n",
            "Processed 900000 records...\n",
            "Processed 1000000 records...\n",
            "Processed 1100000 records...\n",
            "Processed 1200000 records...\n",
            "Processed 1300000 records...\n",
            "Processed 1400000 records...\n",
            "Processed 1500000 records...\n",
            "Processed 1600000 records...\n",
            "Processed 1700000 records...\n",
            "Processed 1800000 records...\n",
            "Processed 1900000 records...\n",
            "Processed 2000000 records...\n",
            "Processed 2100000 records...\n",
            "Processed 2200000 records...\n",
            "Processed 2300000 records...\n",
            "Processed 2400000 records...\n",
            "Processed 2500000 records...\n",
            "Processed 2600000 records...\n",
            "Processed 2700000 records...\n",
            "Processed 2800000 records...\n",
            "Processed 2900000 records...\n",
            "Processed 3000000 records...\n",
            "Processed 3100000 records...\n",
            "Processed 3200000 records...\n",
            "Processed 3300000 records...\n",
            "Processed 3400000 records...\n",
            "Processed 3500000 records...\n",
            "Processed 3600000 records...\n",
            "Processed 3700000 records...\n",
            "Processed 3800000 records...\n",
            "Processed 3900000 records...\n",
            "Processed 4000000 records...\n",
            "Processed 4100000 records...\n",
            "Finished processing 4190001 records.\n",
            "Created dictionary with 979 geocoded records...\n",
            "Writing output for all geo records...\n",
            "Wrote output file - Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, csv, os\n",
        "\n",
        "# The input file is already correctly specified with its full path.\n",
        "infile = '/content/output/all_records_with_geo_v21.json'\n",
        "\n",
        "# Load the JSON data directly from the infile, no need to modify the path.\n",
        "with open(infile, 'r', encoding='utf-8') as json_file:\n",
        "    twit_data = json.load(json_file)\n",
        "\n",
        "twit_list = []\n",
        "\n",
        "# Process the JSON data to extract the needed information.\n",
        "for k, v in twit_data.items():\n",
        "    tweet_id = k\n",
        "    timestamp = v.get('created_at')\n",
        "    tweet = v.get('text')\n",
        "    #source = v.get('source')\n",
        "    #source_url = source.split('\"')[1] if source else None\n",
        "    #source_name = source.split('>')[-1].split('<')[0] if source else None\n",
        "    lang = v.get('lang')\n",
        "    longitude = v.get('geo')['coordinates'][1] if v.get('geo') else None\n",
        "    latitude = v.get('geo')['coordinates'][0] if v.get('geo') else None\n",
        "    country = v.get('place')['country'] if v.get('place') else None\n",
        "    ccode = v.get('place')['country_code'] if v.get('place') else None\n",
        "    #place_sht = v.get('place')['name'] if v.get('place') else None\n",
        "    #place_lng = v.get('place')['full_name'] if v.get('place') else None\n",
        "    #user_id = v.get('user')['id']\n",
        "    #user_name = v.get('user')['name']\n",
        "    #user_desc = v.get('user')['description']\n",
        "    #user_loc = v.get('user')['location']\n",
        "    #user_created = v.get('user')['created_at']\n",
        "    #followers = v.get('user')['followers_count']\n",
        "    #friends = v.get('user')['friends_count']\n",
        "\n",
        "    record = [tweet_id, timestamp, tweet, lang, longitude, latitude, country, ccode]\n",
        "    twit_list.append(record)\n",
        "    #[source_url, source_name, place_sht, place_lng, user_id, user_name, user_desc, user_loc, user_created, followers, friends]\n",
        "\n",
        "\n",
        "outfile = 'november_21st.csv'\n",
        "\n",
        "# Ensure the output directory exists before writing the file.\n",
        "output_dir = 'output'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "outpath = os.path.join(output_dir, outfile)\n",
        "\n",
        "# Write the output CSV file.\n",
        "with open(outpath, 'w', newline='', encoding='utf-8') as writefile:\n",
        "    writer = csv.writer(writefile, quoting=csv.QUOTE_ALL, delimiter=',')\n",
        "    header = ['tweet_id', 'timestamp', 'tweet', 'lang', 'longitude', 'latitude', 'country','ccode']\n",
        "    #['source_url', 'source_name', 'place_sht', 'place_lng', 'user_id', 'user_name', 'user_desc', 'user_loc','user_created', 'followers', 'friends']\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(twit_list)\n",
        "\n",
        "print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHjLmihx5qdH",
        "outputId": "697656a1-98ad-4554-a686-3862d487d180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ]
    }
  ]
}